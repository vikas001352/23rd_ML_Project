{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b734d-198e-4ed2-8382-0ff09cc2b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS-1\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_video_urls(search_query, num_videos=5):\n",
    "    base_url = f\"https://www.youtube.com/results?search_query={search_query.replace(' ', '+')}\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    video_links = soup.select('.yt-uix-tile-link')\n",
    "    video_urls = []\n",
    "\n",
    "    for link in video_links[:num_videos]:\n",
    "        video_url = 'https://www.youtube.com' + link['href']\n",
    "        video_urls.append(video_url)\n",
    "\n",
    "    return video_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the YouTube search query: \")\n",
    "    num_videos = 5\n",
    "    video_urls = extract_video_urls(search_query, num_videos)\n",
    "\n",
    "    if video_urls:\n",
    "        print(\"Video URLs:\")\n",
    "        for index, url in enumerate(video_urls, 1):\n",
    "            print(f\"{index}. {url}\")\n",
    "    else:\n",
    "        print(\"No videos found.\")\n",
    "\n",
    "        \n",
    "        \n",
    "    ANS-2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_thumbnail_urls(search_query, num_videos=5):\n",
    "    base_url = f\"https://www.youtube.com/results?search_query={search_query.replace(' ', '+')}\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    video_links = soup.select('.yt-uix-tile-link')\n",
    "    thumbnail_urls = []\n",
    "\n",
    "    for link in video_links[:num_videos]:\n",
    "        video_url = 'https://www.youtube.com' + link['href']\n",
    "        thumbnail_url = get_thumbnail_url(video_url)\n",
    "        if thumbnail_url:\n",
    "            thumbnail_urls.append(thumbnail_url)\n",
    "\n",
    "    return thumbnail_urls\n",
    "\n",
    "def get_thumbnail_url(video_url):\n",
    "    response = requests.get(video_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the video page: {video_url}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    thumbnail_element = soup.find('meta', property='og:image')\n",
    "\n",
    "    if thumbnail_element and 'content' in thumbnail_element.attrs:\n",
    "        return thumbnail_element['content']\n",
    "\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the YouTube search query: \")\n",
    "    num_videos = 5\n",
    "    thumbnail_urls = extract_thumbnail_urls(search_query, num_videos)\n",
    "\n",
    "    if thumbnail_urls:\n",
    "        print(\"Thumbnail URLs:\")\n",
    "        for index, url in enumerate(thumbnail_urls, 1):\n",
    "            print(f\"{index}. {url}\")\n",
    "    else:\n",
    "        print(\"No thumbnails found.\")\n",
    "\n",
    "        \n",
    "        \n",
    "        ANS-3\n",
    "        \n",
    "        \n",
    "        import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_video_titles(search_query, num_videos=5):\n",
    "    base_url = f\"https://www.youtube.com/results?search_query={search_query.replace(' ', '+')}\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    video_links = soup.select('.yt-uix-tile-link')\n",
    "    video_titles = []\n",
    "\n",
    "    for link in video_links[:num_videos]:\n",
    "        video_title = link['title']\n",
    "        video_titles.append(video_title)\n",
    "\n",
    "    return video_titles\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the YouTube search query: \")\n",
    "    num_videos = 5\n",
    "    video_titles = extract_video_titles(search_query, num_videos)\n",
    "\n",
    "    if video_titles:\n",
    "        print(\"Video Titles:\")\n",
    "        for index, title in enumerate(video_titles, 1):\n",
    "            print(f\"{index}. {title}\")\n",
    "    else:\n",
    "        print(\"No video titles found.\")\n",
    "\n",
    "        \n",
    "        ANS-4\n",
    "        \n",
    "        \n",
    "        import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_video_views(search_query, num_videos=5):\n",
    "    base_url = f\"https://www.youtube.com/results?search_query={search_query.replace(' ', '+')}\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    video_links = soup.select('.yt-uix-tile-link')\n",
    "    video_views = []\n",
    "\n",
    "    for link in video_links[:num_videos]:\n",
    "        video_url = 'https://www.youtube.com' + link['href']\n",
    "        video_view_count = get_video_view_count(video_url)\n",
    "        video_views.append(video_view_count)\n",
    "\n",
    "    return video_views\n",
    "\n",
    "def get_video_view_count(video_url):\n",
    "    response = requests.get(video_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the video page: {video_url}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    view_count_element = soup.find('span', {'class': 'view-count'})\n",
    "\n",
    "    if view_count_element:\n",
    "        return view_count_element.text.strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the YouTube search query: \")\n",
    "    num_videos = 5\n",
    "    video_views = extract_video_views(search_query, num_videos)\n",
    "\n",
    "    if video_views:\n",
    "        print(\"Number of Views:\")\n",
    "        for index, views in enumerate(video_views, 1):\n",
    "            print(f\"{index}. {views}\")\n",
    "    else:\n",
    "        print(\"No view counts found.\")\n",
    "\n",
    "        \n",
    "        \n",
    "        ANS-5\n",
    "        \n",
    "        \n",
    "        \n",
    "        import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_video_views(search_query, num_videos=5):\n",
    "    base_url = f\"https://www.youtube.com/results?search_query={search_query.replace(' ', '+')}\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    video_links = soup.select('.yt-uix-tile-link')\n",
    "    video_views = []\n",
    "\n",
    "    for link in video_links[:num_videos]:\n",
    "        video_url = 'https://www.youtube.com' + link['href']\n",
    "        video_view_count = get_video_view_count(video_url)\n",
    "        video_views.append(video_view_count)\n",
    "\n",
    "    return video_views\n",
    "\n",
    "def get_video_view_count(video_url):\n",
    "    response = requests.get(video_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the video page: {video_url}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    view_count_element = soup.find('span', {'class': 'view-count'})\n",
    "\n",
    "    if view_count_element:\n",
    "        return view_count_element.text.strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the YouTube search query: \")\n",
    "    num_videos = 5\n",
    "    video_views = extract_video_views(search_query, num_videos)\n",
    "\n",
    "    if video_views:\n",
    "        print(\"Number of Views:\")\n",
    "        for index, views in enumerate(video_views, 1):\n",
    "            print(f\"{index}. {views}\")\n",
    "    else:\n",
    "        print(\"No view counts found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
